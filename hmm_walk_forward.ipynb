{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a247fbb0",
   "metadata": {},
   "source": [
    "# Walk-Forward Analysis with Hidden Markov Models\n",
    "\n",
    "This notebook builds a complete walk-forward backtesting engine for a Market Regime-Switching strategy.\n",
    "\n",
    "We start from the core GMM-HMM model (Gaussian Mixture Model - Hidden Markov Model) explored in `hmm_model.py` and extend it to a rolling-window framework to realistically simulate historical performance.\n",
    "\n",
    "## Chapter 1: Data Gathering & Initialization\n",
    "\n",
    "Our first step is to configure the environment, load the financial data, and inspect it to ensure it is suitable for a long-term walk-forward analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from stock import Stock\n",
    "from typing import Tuple, List, Dict, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6697093",
   "metadata": {},
   "source": [
    "### 1.1 Configuration\n",
    "\n",
    "We select the asset and the time frequency. For a robust walk-forward test, we need a long history of data. \n",
    "We also define the `TRAINING_WINDOW`, which determines how much past data the model 'sees' at each step to learn the regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "TICKER = 'AAPL'            # Asset to backtest\n",
    "TIME_FREQUENCY = 'weekly'  # 'daily' or 'weekly'\n",
    "\n",
    "# Walk-Forward Parameters\n",
    "# How many periods of history to use for training the HMM at each step\n",
    "# Dynamic logic: 500 weeks (~10 years) or 1500 days (~6 years)\n",
    "if TIME_FREQUENCY == 'weekly':\n",
    "    TRAINING_WINDOW = 500\n",
    "else:\n",
    "    TRAINING_WINDOW = 1500\n",
    "\n",
    "print(f\"Configuration: {TICKER} [{TIME_FREQUENCY}]\")\n",
    "print(f\"Rolling Window Size: {TRAINING_WINDOW} periods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15af313",
   "metadata": {},
   "source": [
    "### 1.2 Data Loading\n",
    "\n",
    "We use the project's `Stock` class to fetch data from Yahoo Finance. We ensure we have enough history to support both the initial training window and a significant out-of-sample testing period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b184bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load data\n",
    "my_stock = Stock(TICKER, TIME_FREQUENCY)\n",
    "# Load as much history as possible (e.g. from 1990)\n",
    "my_stock.load_data(start_date='1980-01-01')\n",
    "\n",
    "# Extract key series\n",
    "df = my_stock.get_data()\n",
    "prices = df['Close']\n",
    "log_returns = df['Log_Returns'].dropna()\n",
    "\n",
    "print(f\"Data Loaded: {len(log_returns)} observations\")\n",
    "print(f\"Date Range: {log_returns.index[0].date()} to {log_returns.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b9365",
   "metadata": {},
   "source": [
    "### 1.3 Inspection for Walk-Forward Validity\n",
    "\n",
    "Before we proceed to modeling, we visualize the full history. \n",
    "1. **Price History (Log Scale)**: Verifies the long-term trend and data quality.\n",
    "2. **Returns Distribution**: Confirms the non-Normal characteristics (fat tails) that justify using an HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812ef12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Visualize Price History\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.semilogy(prices.index, prices.values, label=f'{TICKER} Close')\n",
    "plt.title(f\"{TICKER} Price History (Log Scale)\")\n",
    "plt.ylabel(\"Price ($)\")\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'${y:.0f}'))\n",
    "ax.minorticks_off()  # Remove minor tick marks\n",
    "plt.grid(True, which=\"major\", ls=\"-\", alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20bf7e9",
   "metadata": {},
   "source": [
    "### Empirical Distribution Analysis\n",
    "\n",
    "Here we visually examine the distribution of log-returns. The red line represents a standard Normal distribution fitted to the data.\n",
    "Note the discrepancies: the empirical data has generally a higher peak and much wider \"tails\" (extreme values) than the Normal curve predicts.\n",
    "This motivates using a **Mixture Model** (combination of Gaussians) rather than a simple Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data: pd.Series, multiplier: int = 4):\n",
    "    \"\"\"\n",
    "    Filter out extreme outliers for cleaner visualization (using IQR method).\n",
    "    This helps in seeing the main body of the distribution without the graph being distorted by 1-2 extreme crashes.\n",
    "    \"\"\"\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return data[(data >= Q1 - multiplier * IQR) & (data <= Q3 + multiplier * IQR)]\n",
    "\n",
    "# Filter the returns from outliers ONLY for a better visualization\n",
    "filtered_returns = remove_outliers(log_returns, multiplier=5)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Histogram vs Normal\n",
    "sns.histplot(filtered_returns, bins=100, stat='density', alpha=0.6, color='steelblue', label='Empirical Data', ax=ax1)\n",
    "x_range = np.linspace(filtered_returns.min(), filtered_returns.max(), 100)\n",
    "stats_mu, stats_sigma = filtered_returns.mean(), filtered_returns.std()\n",
    "ax1.plot(x_range, stats.norm.pdf(x_range, stats_mu, stats_sigma), \n",
    "        'r--', lw=2, label=f'Normal ($\\mu$={stats_mu:.4f}, $\\sigma$={stats_sigma:.4f})')\n",
    "ax1.set_title(f'Returns Distribution ({TICKER})')\n",
    "ax1.legend()\n",
    "\n",
    "# Q-Q Plot\n",
    "# Deviations from the red line indicate non-normality (fat tails)\n",
    "stats.probplot(filtered_returns, dist=\"norm\", plot=ax2)\n",
    "ax2.get_lines()[0].set_markerfacecolor('steelblue')\n",
    "ax2.get_lines()[0].set_markersize(4)\n",
    "ax2.set_title('Q-Q Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quick sanity check for Walk-Forward\n",
    "total_observations = len(log_returns)\n",
    "max_cycles = total_observations - TRAINING_WINDOW\n",
    "if max_cycles <= 0:\n",
    "    raise ValueError(f\"Insufficient data. Need > {TRAINING_WINDOW} periods, but have {total_observations}.\")\n",
    "    \n",
    "print(f\"Data Check: {total_observations} obs. Sufficient for {max_cycles} Out-of-Sample predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c8552",
   "metadata": {},
   "source": [
    "## Chapter 2: The Identification Problem & Economic Sorting\n",
    "\n",
    "### The Theoretical Challenge: Label Switching\n",
    "\n",
    "In finite mixture models and Hidden Markov Models, we encounter the **Identification Problem** (often referred to as \"Label Switching\").\n",
    "The Likelihood function of the model is invariant to the permutation of the latent states.\n",
    "This means that if we simply swap the parameters of \"State 0\" and \"State 1\" (e.g., exchange their means, variances, and transition probabilities), the model explains the data equally well. The statistical algorithm has no intrinsic concept that \"State 0\" should represent a Bear market and \"State 1\" a Bull market.\n",
    "\n",
    "In the context of a **Walk-Forward (Rolling Window) Analysis**, this is critical.\n",
    "*   **Window $T$**: The model might identify \"State 0\" as the Low-Volatility regime and \"State 1\" as High-Volatility.\n",
    "*   **Window $T+1$**: The numerical optimization might swap these, identifying \"State 0\" as High-Volatility.\n",
    "\n",
    "Without correction, concatenating the state predictions across windows results in an incoherent time series, rendering the backtest invalid.\n",
    "\n",
    "### Solution: Imposing a Statistical Ordering\n",
    "\n",
    "To resolve this, we must strictly order the states based on an invariant economic metric—a **Utility Function** $U(\\cdot)$—such that for our sorted states:\n",
    "$$ U(\\text{State}_0) < U(\\text{State}_1) < \\dots < U(\\text{State}_{K-1}) $$\n",
    "\n",
    "We define this function to sort regimes from \"Worst\" (Bear/Crash) to \"Best\" (Bull/Stable).\n",
    "\n",
    "**Why Mean-Variance is usually enough (but we go further)**:\n",
    "A standard **Mean-Variance** utility is a robust and effective way to distinguish regimes. High returns and low volatility clearly indicate a \"Bull\" state, while negative returns and high volatility indicate a \"Bear\" state.\n",
    "\n",
    "However, adding **Skewness** and **Kurtosis** to the objective function can provide an extra layer of precision. This helps specifically in \"close calls\"—for example, distinguishing between a volatile recovery (high vol, positive skew) and a volatile crash (high vol, negative skew).\n",
    "\n",
    "We use a **Higher-Moment Utility Proxy**:\n",
    "\n",
    "$$ U(s) = \\mu_s - \\frac{1}{2}\\sigma_s^2 + \\lambda_1 \\cdot \\text{Skew}_s - \\lambda_2 \\cdot \\text{Kurt}_s $$\n",
    "\n",
    "*   **$\\mu_s, \\sigma_s^2$**: The aggregated mean and variance of the Gaussian Mixture for state $s$.\n",
    "*   **$\\text{Skew}_s$**: Captures asymmetry (Crash Risk).\n",
    "*   **$\\text{Kurt}_s$**: Captures tail thickness (Fat Tails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GMMHMM\n",
    "\n",
    "def calculate_state_moments(model: GMMHMM, state_idx: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates the first four moments (Mean, Variance, Skewness, Kurtosis) \n",
    "    for a specific state's Gaussian Mixture distribution.\n",
    "\n",
    "    This function aggregates the parameters of the component Gaussians within a state \n",
    "    using the Law of Total Cumulants/Moments.\n",
    "\n",
    "    :param model: The trained Hidden Markov Model.\n",
    "    :param state_idx: The index of the hidden state to analyze.\n",
    "\n",
    "    :return: A dictionary containing:\n",
    "        - 'mean': Aggregated expected return.\n",
    "        - 'var': Aggregated variance.\n",
    "        - 'skew': Fisher-Pearson coefficient of skewness.\n",
    "        - 'kurt': Excess kurtosis.\n",
    "    \"\"\"\n",
    "    # 1. Extract mixture parameters for the specific state\n",
    "    weights = model.weights_[state_idx]\n",
    "    means = model.means_[state_idx].flatten()\n",
    "    \n",
    "    # Handle covariance types\n",
    "    if model.covariance_type == 'tied':\n",
    "        # One shared covariance matrix for all states/components\n",
    "        covs = np.full(len(weights), model.covars_[state_idx][0][0])\n",
    "    elif model.covariance_type == 'diag':\n",
    "        # Diagonal covariance per component\n",
    "        covs = model.covars_[state_idx].flatten()\n",
    "    else:\n",
    "        # Full covariance, extracting diagonal elements for 1D approximation\n",
    "        covs = model.covars_[state_idx].flatten()\n",
    "\n",
    "    # 2. Aggregated Mean: E[X]\n",
    "    # Simple weighted sum of component means\n",
    "    agg_mean = np.sum(weights * means)\n",
    "\n",
    "    # 3. Aggregated Variance: Law of Total Variance\n",
    "    # Var(X) = E[Var(X|Z)] + Var(E[X|Z])\n",
    "    #        = sum(w * sigma^2) + sum(w * (mu - agg_mean)^2)\n",
    "    agg_var = np.sum(weights * covs) + np.sum(weights * (means - agg_mean)**2)\n",
    "    \n",
    "    # Numerical stability check\n",
    "    if agg_var < 1e-12:\n",
    "        return {'mean': agg_mean, 'var': 0.0, 'skew': 0.0, 'kurt': 0.0}\n",
    "    \n",
    "    # 4. Higher Moments via Central Moments\n",
    "    # Deviation of component means from aggregate mean\n",
    "    d = means - agg_mean\n",
    "    \n",
    "    # 3rd Central Moment (Skewness numerator): E[(X - mu)^3]\n",
    "    # approx: sum(w * (d^3 + 3*d*sigma^2))\n",
    "    m3 = np.sum(weights * (d**3 + 3 * d * covs))\n",
    "    \n",
    "    # 4th Central Moment (Kurtosis numerator): E[(X - mu)^4]\n",
    "    # approx: sum(w * (d^4 + 6*d^2*sigma^2 + 3*sigma^4))\n",
    "    m4 = np.sum(weights * (d**4 + 6 * (d**2) * covs + 3 * (covs**2)))\n",
    "    \n",
    "    # Standardize\n",
    "    skew = m3 / (agg_var ** 1.5)\n",
    "    kurt = (m4 / (agg_var ** 2)) - 3.0  # Excess kurtosis\n",
    "\n",
    "    return {\n",
    "        'mean': agg_mean,\n",
    "        'var': agg_var,\n",
    "        'skew': skew,\n",
    "        'kurt': kurt\n",
    "    }\n",
    "\n",
    "def get_utility_score(\n",
    "    moments: Dict[str, float], \n",
    "    skew_weight: float = 0.5, \n",
    "    kurt_weight: float = 0.1\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes a risk-adjusted utility score from distributional moments.\n",
    "    \n",
    "    Score = Mean - 0.5 * Variance + Skew_Penalty - Kurt_Penalty\n",
    "\n",
    "    :param moments: Dictionary with 'mean', 'var', 'skew', 'kurt'.\n",
    "    :param skew_weight: Weight for skewness preference. Positive value rewards positive skew.\n",
    "    :param kurt_weight: Weight for kurtosis penalty. Positive value penalizes fat tails.\n",
    "\n",
    "    :return: The computed utility score.\n",
    "    \"\"\"\n",
    "    # 1. Mean-Variance Base (Certainty Equivalent approximation)\n",
    "    score = moments['mean'] - 0.5 * moments['var']\n",
    "    \n",
    "    # 2. Skewness Adjustment\n",
    "    # We penalize negative skewness (crash risk). \n",
    "    # If skew is positive, we add to score. If negative, we subtract.\n",
    "    score += skew_weight * moments['skew']\n",
    "    \n",
    "    # 3. Kurtosis Adjustment\n",
    "    # We penalize excess kurtosis (uncertainty/fat tails).\n",
    "    # Higher kurtosis reduces the score.\n",
    "    score -= kurt_weight * moments['kurt']\n",
    "    \n",
    "    return score\n",
    "\n",
    "def sort_states_by_utility(\n",
    "    model: GMMHMM, \n",
    "    skew_weight: float = 0.25, \n",
    "    kurt_weight: float = 0.10\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Determines the sorting permutation that ranks states from Lowest Utility to Highest Utility.\n",
    "\n",
    "    :param model: The trained HMM model.\n",
    "    :param skew_weight: Importance of skewness in utility.\n",
    "    :param kurt_weight: Importance of kurtosis in utility.\n",
    "\n",
    "    :return: An array of indices representing the sorted order.\n",
    "             e.g. [2, 0, 1] means State 2 is Worst, State 0 is Neutral, State 1 is Best.\n",
    "    \"\"\"\n",
    "    n_states = model.n_components\n",
    "    scores = []\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        moments = calculate_state_moments(model, s)\n",
    "        u = get_utility_score(moments, skew_weight, kurt_weight)\n",
    "        scores.append(u)\n",
    "        \n",
    "    # Return indices that sort the scores in ascending order\n",
    "    return np.argsort(scores)\n",
    "\n",
    "def apply_state_permutation(model: GMMHMM, perm: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Reorders the internal parameters of the GMMHMM model in-place according to a permutation.\n",
    "    \n",
    "    This ensures that 'State 0' always corresponds to the first index in 'perm',\n",
    "    'State 1' to the second, etc.\n",
    "\n",
    "    :param model: The model to modify.\n",
    "    :param perm: The new order of states (e.g. [2, 0, 1]).\n",
    "    \"\"\"\n",
    "    if np.all(perm == np.arange(len(perm))): \n",
    "        return\n",
    "\n",
    "    # 1. Reorder Initial Probabilities (startprob_)\n",
    "    model.startprob_ = model.startprob_[perm]\n",
    "\n",
    "    # 2. Reorder Transition Matrix (transmat_)\n",
    "    # We must reorder both rows (from_state) and columns (to_state)\n",
    "    model.transmat_ = model.transmat_[perm][:, perm]\n",
    "\n",
    "    # 3. Reorder Emission Parameters (weights, means, covars)\n",
    "    # These have shape (n_comp, n_mix, ...) so we just reorder the first axis\n",
    "    model.weights_ = model.weights_[perm]\n",
    "    model.means_ = model.means_[perm]\n",
    "    model.covars_ = model.covars_[perm]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "markov_switch",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
